{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file is parsing i2b2 2010 training data and annotating it with the CoNLL BIO scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import pos_tag, RegexpParser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: nltk: not found\n"
     ]
    }
   ],
   "source": [
    "!nltk.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: all anotations have a corresponding entry. 73\n"
     ]
    }
   ],
   "source": [
    "a_ids = []\n",
    "e_ids = []\n",
    "\n",
    "for filename in os.listdir(\"./data/annotations\"):\n",
    "    if filename[0] != \".\":  # ignore hidden files\n",
    "        a_ids.append(int(filename))\n",
    "for filename in os.listdir(\"./data/entries\"):\n",
    "    if filename[0] != \".\": \n",
    "        e_ids.append(int(filename))\n",
    "    \n",
    "a_ids = tuple(sorted(a_ids)) \n",
    "e_ids = tuple(sorted(e_ids))\n",
    "\n",
    "intersection = list(set(a_ids) & set(e_ids))\n",
    "if len(intersection) == len(a_ids):\n",
    "    print(\"Success: all anotations have a corresponding entry.\", len(intersection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build annotation and entry corpora\n",
    "\n",
    "a_corpus = []\n",
    "e_corpus = []\n",
    "entries_txt = []\n",
    "entries_list = []\n",
    "\n",
    "# only annotations and corresponding files\n",
    "for file in a_ids:\n",
    "    path = \"./data/annotations/\" + str(file)\n",
    "    with open(path) as f:\n",
    "        content = f.read().splitlines()\n",
    "        a_corpus.append(content)\n",
    "\n",
    "    path = \"./data/entries/\" + str(file)\n",
    "    with open(path) as f:\n",
    "        #content = f.readlines()\n",
    "        file_read = f.read()\n",
    "        entries_list.append((file, file_read))\n",
    "        entries_txt.append(file_read)\n",
    "        \n",
    "        content = file_read.splitlines()\n",
    "        e_corpus.append(content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Admission Date :',\n",
       " '2018-10-25',\n",
       " 'Discharge Date :',\n",
       " '2018-10-31',\n",
       " 'Date of Birth :',\n",
       " '1951-06-15',\n",
       " 'Sex :',\n",
       " 'M',\n",
       " 'Service : ',\n",
       " 'CARDIOTHORACIC',\n",
       " 'Allergies :',\n",
       " 'Patient recorded as having No Known Allergies to Drugs',\n",
       " 'Attending : Michael D. Christensen , M.D.',\n",
       " 'Chief Complaint :',\n",
       " 'Shortness of Breath',\n",
       " 'Major Surgical or Invasive Procedure :',\n",
       " 'Coronary Artery Bypass Graft x3 ( Left internal mammary -> left anterior descending , saphaneous vein graft -> obtuse marginal , saphaneous vein graft -> posterior descending artery ) 2018-10-25',\n",
       " 'History of Present Illness :',\n",
       " '67 y/o male with worsening shortness of breath. Had abnormal ETT and referred for cath .',\n",
       " 'Cath revealed severe 3 vessel disease .',\n",
       " 'Then referred for surgical intervention .',\n",
       " 'Past Medical History :',\n",
       " \"Carpal tunnel syndrome , Hypertension , Hyperlipidemia , Arthritis , h/o Bell's Palsy , HOH , s/p Tonsillectomy\",\n",
       " 'Social History :',\n",
       " 'Denies ETOH , rare Tobacco .',\n",
       " 'Electrician .',\n",
       " 'Family History :',\n",
       " \"Father with MI in 50 's and underwent CABG .\",\n",
       " 'Physical Exam :',\n",
       " 'VS : 65 20 160/100 5 \\' 7 \" 180 #',\n",
       " 'General : WD/WN male in NAD',\n",
       " 'HEENT : EOMI , PERRL , NC/AT',\n",
       " 'Neck : Supple , FROM , - JVD , - carotid bruits',\n",
       " 'Chest : CTAB - w/r/r',\n",
       " 'Heart : RRR - c/r/m/g',\n",
       " 'Abd : Soft , NT/ND +BS',\n",
       " 'Ext : Warm , well-perfused - edema , - varicosities',\n",
       " 'Neuro : A&Ox3 , MAE , non-focal',\n",
       " 'Pertinent Results :',\n",
       " 'Echo 10-25 : PRE-BYPASS : Left ventricular wall thicknesses are normal .',\n",
       " 'The left ventricular cavity size is normal .',\n",
       " 'There is mild inferior wall hypokinesis .',\n",
       " 'There is akinesis / dyskinesis and thinning of the mid to distal inferior septum and the apex .',\n",
       " 'Overall left ventricular systolic function is mildly depressed .',\n",
       " 'Right ventricular chamber size and free wall motion are normal .',\n",
       " 'The descending thoracic aorta is mildly dilated .',\n",
       " 'There are simple atheroma in the descending thoracic aorta .',\n",
       " 'There are three aortic valve leaflets .',\n",
       " 'There is no aortic valve stenosis .',\n",
       " 'No aortic regurgitation is seen .',\n",
       " 'The mitral valve leaflets are mildly thickened .',\n",
       " 'Trivial mitral regurgitation is seen .',\n",
       " 'POST-BYPASS : LV and RV function is unchanged .',\n",
       " 'Aorta is unchanged .',\n",
       " 'Other findings are unchanged .',\n",
       " 'CXR 10-30 : Left lower lobe atelectasis has partially cleared .',\n",
       " 'Upper lungs are clear .',\n",
       " 'Mild postoperative widening of the cardiomediastinal silhouette is stable .',\n",
       " 'No pneumothorax .',\n",
       " '2018-10-25 11:15 AM BLOOD WBC - 18.3 *# RBC - 3.42 * Hgb - 10.9 * Hct - 31.6 * MCV - 92 MCH - 31.7 MCHC - 34.4 RDW - 13.3 Plt Ct - 134 *',\n",
       " '2018-10-31 06:25 AM BLOOD WBC - 13.6 * RBC - 2.72 * Hgb - 8.6 * Hct - 24.6 * MCV - 91 MCH - 31.7 MCHC - 35.0 RDW - 14.0 Plt Ct - 314',\n",
       " '2018-10-25 11:15 AM BLOOD PT - 13.3 * PTT - 30.0 INR(PT) - 1.2 *',\n",
       " '2018-10-29 06:50 AM BLOOD PT - 11.9 INR(PT) - 1.0',\n",
       " '2018-10-25 12:36 PM BLOOD UreaN - 17 Creat - 0.7 Cl - 111 * HCO3 - 23',\n",
       " '2018-10-31 06:25 AM BLOOD Glucose - 91 UreaN - 19 Creat - 0.8 Na - 134 K - 4.0 Cl - 98 HCO3 - 26 AnGap - 14',\n",
       " 'Brief Hospital Course :',\n",
       " 'Mr. Kammerer was a same day admit and on 10-25 was brought to the operating room where he underwent a coronary artery bypass graft x 3 .',\n",
       " 'Please see operative report for surgical details .',\n",
       " 'He tolerated the procedure well and was transferred to the CSRU for invasive monitoring in stable condition .',\n",
       " 'Later on op day he was weaned from sedation , awoke neurologically intact , and ',\n",
       " 'extubated .',\n",
       " 'Beta blockers and diuretics were initiated on post-op day one .',\n",
       " 'He was diuresed towards his pre-op weight .',\n",
       " 'He appeared to be doing well and was transferred to the SDU on this day .',\n",
       " 'He did have burst of atrial fibrillation and was started on a Amiodarone gtt .',\n",
       " 'His beta blockers were also titrated for maximal BP and HR control .',\n",
       " 'Chest tubes were removed on post-op day two and epicardial pacing wires on post-op day three .',\n",
       " 'Over the next several days he continued to improve his ambulation and mobility with physical therapy .',\n",
       " 'He had no further episodes of AFIB while on po Amiodarone .',\n",
       " 'On post-op day five he appeared to have left arm phlebitis and was started on antibiotics .',\n",
       " 'He was discharged home on post-op day six with antibiotics and appropriate meds .',\n",
       " 'He will have VNA services and make the appropriate follow-up appointments .',\n",
       " 'Medications on Admission :',\n",
       " 'Aspirin 81 mg qd , Toprol XL 50 mg qd , MVI , Flexeril 10 mg qhs , Plavix 75 mg qd ( last on 10-18 )',\n",
       " 'Discharge Medications :',\n",
       " '1. Docusate Sodium 100 mg Capsule Sig : One ( 1 ) Capsule PO BID ( 2 times a day ).',\n",
       " 'Disp :* 60 Capsule (s)* Refills :* 0 *',\n",
       " '2. Aspirin 81 mg Tablet , Delayed Release ( E.C. ) Sig : One ( 1 ) Tablet , Delayed Release ( E.C. ) PO DAILY ( Daily ).',\n",
       " 'Disp:* 30 Tablet , Delayed Release ( E.C. )(s)* Refills :* 0 *',\n",
       " '3. Ranitidine HCl 150 mg Tablet Sig : One ( 1 ) Tablet PO BID ( 2 times a day ).',\n",
       " 'Disp :* 60 Tablet (s)* Refills :* 0 *',\n",
       " '4. Oxycodone - Acetaminophen 5-325 mg Tablet Sig : 1-2 Tablets PO Q4H ( every 4 hours ) as needed for pain .',\n",
       " 'Disp :* 40 Tablet (s)* Refills :* 0 *',\n",
       " '5. Atorvastatin 10 mg Tablet Sig : One ( 1 ) Tablet PO DAILY ( Daily ).',\n",
       " 'Disp :* 30 Tablet (s)* Refills :* 0 *',\n",
       " '6. Amiodarone 200 mg Tablet Sig : Two ( 2 ) Tablet PO BID ( 2 times a day ): please take 400 mg twice a day until 11-02 then decrease to 400 mg once a day for 1 week and then decrease to 200 mg once a day .',\n",
       " 'Disp :* 40 Tablet (s)* Refills :* 0 *',\n",
       " '7. Metoprolol Succinate 50 mg Tablet Sustained Release 24 HR Sig : One ( 1 ) Tablet Sustained Release 24 HR PO DAILY ( Daily ).',\n",
       " 'Disp :* 30 Tablet Sustained Release 24 HR (s)* Refills :* 0 *',\n",
       " '8. Ferrous Gluconate 300 mg Tablet Sig : One ( 1 ) Tablet PO DAILY ( Daily ).',\n",
       " 'Disp :* 30 Tablet (s)* Refills :* 0 *',\n",
       " '9. Ascorbic Acid 500 mg Tablet Sig : One ( 1 ) Tablet PO BID ( 2 times a day ).',\n",
       " 'Disp :* 60 Tablet (s)* Refills :* 0 *',\n",
       " '10. Cephalexin 500 mg Capsule Sig : One ( 1 ) Capsule PO Q6H ( every 6 hours ) for 6 days : please complete full course .',\n",
       " 'Disp :* 24 Capsule (s)* Refills :* 0 *',\n",
       " '11. Furosemide 20 mg Tablet Sig : One ( 1 ) Tablet PO once a day for 2 weeks .',\n",
       " 'Disp :* 14 Tablet (s)* Refills :* 0 *',\n",
       " '12. Potassium Chloride 10 mEq Capsule , Sustained Release Sig : One ( 1 ) Capsule , Sustained Release PO DAILY ( Daily ) for 2 weeks .',\n",
       " 'Disp :* 14 Capsule , Sustained Release (s)* Refills :* 0 *',\n",
       " '13. Lisinopril 5 mg Tablet Sig : 0.5 Tablet PO DAILY ( Daily ).',\n",
       " 'Disp :* 15 Tablet (s)* Refills :* 0 *',\n",
       " 'Discharge Disposition :',\n",
       " 'Home with Service',\n",
       " 'Discharge Diagnosis :',\n",
       " 'Coronary artery disease s/p Coronary Artery Bypass Graft x3',\n",
       " 'PMH : Carpal tunnel syndrome , Hypertension , Hyperlipidemia , Arthritis',\n",
       " 'Discharge Condition :',\n",
       " 'good',\n",
       " 'Discharge Instructions :',\n",
       " 'May shower , no baths or swimming',\n",
       " 'Monitor wounds for infection - redness , drainage , or increased pain',\n",
       " 'Report any fever greater than 101',\n",
       " 'Report any weight gain of greater than 2 pounds in 24 hours or 5 pounds in a week',\n",
       " 'No creams , lotions , powders , or ointments to incisions',\n",
       " 'No driving for approximately one month',\n",
       " 'No lifting more than 10 pounds for 10 weeks',\n",
       " 'Please call with any questions or concerns',\n",
       " 'Left Arm continue with elevation , ice , and complete all antibiotics',\n",
       " 'Followup Instructions :',\n",
       " 'Dr Day in 4 weeks (( 469 ) 587-0462 ) please call for appointment',\n",
       " 'Dr Miller in 1 week ( 262-575-0600 ) please call for appointment',\n",
       " 'Dr Scott Raines in 2-3 weeks please call for appointment',\n",
       " 'Wound check appointment 4 East as instructed by nurse ( 280-5326 )',\n",
       " 'For surgical incision and left arm',\n",
       " 'Craig Douglas MD 10-898',\n",
       " 'Completed by : Reyes Marengo PA 80 - BMD 2018-11-19 @ 1536',\n",
       " 'Signed electronically by : DR. Jeffrey Collins on : WED 2018-11-21 9:28 AM',\n",
       " '( End of Report )']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS_tagger with SparkNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import sparknlp\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38225)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aminmoradi/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aminmoradi/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:38225)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-38698c100ea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m  \u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark://sddm.europe-west1-b.c.rectified-259021.internal:8080'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m  \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Spark NLP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessionState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetConfString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    981\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \"\"\"\n\u001b[0;32m--> 983\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    936\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:38225)"
     ]
    }
   ],
   "source": [
    "# spark = sparknlp.start()\n",
    "spark = SparkSession.builder \\\n",
    " .master('spark://sddm.europe-west1-b.c.rectified-259021.internal:8080') \\\n",
    " .appName('Spark NLP').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ann_pipeline():\n",
    "    \n",
    "    document_assembler = DocumentAssembler() \\\n",
    "        .setInputCol(\"text\")\\\n",
    "        .setOutputCol('document')\n",
    "\n",
    "    sentence = SentenceDetector()\\\n",
    "        .setInputCols(['document'])\\\n",
    "        .setOutputCol('sentence')\\\n",
    "        .setCustomBounds(['\\n'])\\\n",
    "        .setUseCustomBoundsOnly(True)\n",
    "\n",
    "    tokenizer = Tokenizer() \\\n",
    "        .setInputCols([\"sentence\"]) \\\n",
    "        .setOutputCol(\"token\")\n",
    "    \n",
    "    normalizer = Normalizer() \\\n",
    "        .setInputCols([\"token\"]) \\\n",
    "        .setOutputCol(\"normalized\")\\\n",
    "        .setLowercase(True)\\\n",
    "        .setCleanupPatterns([\"[.]\"])\n",
    "\n",
    "    pos = PerceptronModel.pretrained() \\\n",
    "        .setInputCols([\"sentence\", \"normalized\"]) \\\n",
    "        .setOutputCol(\"pos\")\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        stages = [\n",
    "            document_assembler,\n",
    "            sentence,\n",
    "            tokenizer,\n",
    "            normalizer,\n",
    "            pos\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    #empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "    #pipelineFit = pipeline.fit(empty_data)\n",
    "    \n",
    "    #lp_pipeline = LightPipeline(pipelineFit)\n",
    "\n",
    "    print (\"Spark NLP Pipeline is created\")\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(entries_list, columns=['id', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>Admission Date :\\n2018-10-25\\nDischarge Date :...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>Admission Date :\\n2011-03-10\\nDischarge Date :...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>Admission Date :\\nDischarge Date :\\n2014-01-24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>Admission Date :\\n2015-10-28\\nDischarge Date :...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>Admission Date:\\n2011-02-08\\nDischarge Date :\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0  13  Admission Date :\\n2018-10-25\\nDischarge Date :...\n",
       "1  14  Admission Date :\\n2011-03-10\\nDischarge Date :...\n",
       "2  15  Admission Date :\\nDischarge Date :\\n2014-01-24...\n",
       "3  16  Admission Date :\\n2015-10-28\\nDischarge Date :...\n",
       "4  17  Admission Date:\\n2011-02-08\\nDischarge Date :\\..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.createDataFrame(df).toDF('id', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 4.3 MB\n",
      "[OK!]\n",
      "Spark NLP Pipeline is created\n"
     ]
    }
   ],
   "source": [
    "conll_pipeline = get_ann_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = conll_pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = pipeline.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_pos_tag = parsed.select('id', F.explode(F.arrays_zip('normalized.result', 'pos.result', 'normalized.begin', 'normalized.end')).alias(\"cols\")) \\\n",
    "                        .select(F.expr(\"id\"),\n",
    "                                F.expr(\"cols['0']\").alias(\"normalized\"),\n",
    "                                F.expr(\"cols['1']\").alias(\"pos_tag\"),\n",
    "                                F.expr(\"cols['2']\").alias(\"normalized_begin\"),\n",
    "                                F.expr(\"cols['3']\").alias(\"normalized_end\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_end_list = parsed_pos_tag.select('id', 'normalized_end').withColumn('row', F.lit(0)).toPandas().values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sentence = parsed.select('id', F.explode(F.arrays_zip('sentence.result', 'sentence.begin','sentence.end')).alias('cols'))\\\n",
    "                                        .select('id',\n",
    "                                                F.expr(\"cols['0']\").alias(\"sentence\"),\n",
    "                                                F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "                                                F.expr(\"cols['2']\").alias(\"end\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(parsed_sentence['id']).orderBy(parsed_sentence['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sentence_row = parsed_sentence.withColumn('row', F.row_number().over(windowSpec)).orderBy('id', 'row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sentence_row.select('sentence', 'begin', 'end' ).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_pos_df = parsed_pos_tag.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_end_list = parsed_sentence_row.select('id', 'end', 'row').toPandas().values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_counter_token = 0\n",
    "\n",
    "for i in range(len(sentence_end_list)):\n",
    "    \n",
    "    while(id_counter_token < len(token_end_list) and token_end_list[id_counter_token][0] == sentence_end_list[i][0]):\n",
    "        if(token_end_list[id_counter_token][1] <= sentence_end_list[i][1]):\n",
    "            token_end_list[id_counter_token][2] = sentence_end_list[i][2]\n",
    "            id_counter_token += 1\n",
    "            \n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_df = pd.DataFrame(token_end_list, columns= ['id', 'normalized_end', 'row'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_pos_tag_merged = parsed_pos_df.merge(row_df, on = ['id', 'normalized_end'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_pos_tag_merged['offset'] = parsed_pos_tag_merged.groupby(['id', 'row']).cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_pos_tag_merged[450:500]\n",
    "#parsed_pos_tag_final[-10000:-9950]\n",
    "#parsed_pos_tag_final[-10800:-10750]\n",
    "#parsed_pos_tag_merged[70:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER_tagger on i2b2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs = list(nlp.pipe(entries_txt, disable=[\"tagger\", \"parser\", \"ner\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_cols = [\"id\", \"row\", \"offset\", \"token\"]\n",
    "entries_df = pd.DataFrame(columns=entries_cols)\n",
    "entries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_cols = [\"id\", \"NER_tag\", \"row\", \"offset\", \"length\"]\n",
    "annotations_df = pd.DataFrame(columns=annotations_cols)\n",
    "annotations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build annotations data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = pd.DataFrame(columns=annotations_cols)  # reset df\n",
    "tmp_list = []\n",
    "\n",
    "for i, document in enumerate(a_corpus):\n",
    "    \n",
    "    for row in document:\n",
    "        row = row.split(\"||\")\n",
    "        # print(row, \"\\n\")\n",
    "        \n",
    "        label = row[1].split(\"=\")[1][1:-1] # label = treatment\n",
    "        \n",
    "        tag = row[0].split(\"=\")\n",
    "        if \":\" in tag[1]:\n",
    "            tag_row_a = tag[1].split(\" \")[-2:][0].split(\":\")[0] # line_a i.e. 80\n",
    "            tag_row_b = tag[1].split(\" \")[-2:][1].split(\":\")[0] # line_b i.e. 80\n",
    "\n",
    "            # some annotations have non-standard formatting (losing 64 instances)\n",
    "            try:\n",
    "                tag_offset_a = int(tag[1].split(\" \")[-2:][0].split(\":\")[1]) # word_a i.e. 4\n",
    "                tag_offset_b = int(tag[1].split(\" \")[-2:][1].split(\":\")[1]) # word_b i.e. 7\n",
    "                length = tag_offset_b - tag_offset_a + 1\n",
    "\n",
    "                # 1 row = 1 token with a tag\n",
    "                first = True\n",
    "                BIO_tag = \"B-\"\n",
    "                if length > 1 and tag_row_a == tag_row_b:\n",
    "                    for offset in range(tag_offset_a, tag_offset_b+1):\n",
    "                        if first: \n",
    "                            tag_label = BIO_tag + label\n",
    "                            first = False\n",
    "                        else:\n",
    "                            tag_label = tag_label.replace(\"B-\", \"I-\")\n",
    "                        tmp_list.append([a_ids[i], tag_label, tag_row_a, offset, 1])\n",
    "                # TODO: tags over line breaks\n",
    "                else:\n",
    "                    tmp_list.append([a_ids[i], BIO_tag + label, tag_row_a, tag_offset_a, length])\n",
    "            except:\n",
    "                pass             \n",
    "\n",
    "annotations_df = pd.DataFrame(tmp_list, columns=annotations_cols)\n",
    "annotations_df.reset_index(inplace=True)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = annotations_df.drop(columns=[\"index\", \"length\"])\n",
    "annotations_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joing entries and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure correct dtypes\n",
    "annotations_df[['id', 'row', 'offset']] = annotations_df[['id', 'row', 'offset']].apply(pd.to_numeric)\n",
    "annotations_df['NER_tag'] = annotations_df[\"NER_tag\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_pos_tag_merged[['id', 'row', 'offset']] = parsed_pos_tag_merged[['id', 'row', 'offset']].apply(pd.to_numeric)\n",
    "parsed_pos_tag_merged[\"normalized\"] = parsed_pos_tag_merged[\"normalized\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.merge(parsed_pos_tag_merged, annotations_df, how=\"left\", on=['id', 'row', 'offset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaNs with \"O\"\n",
    "print(\"columns with missing data:\\n\", result_df.isna().any())\n",
    "result_df = result_df.fillna(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"columns with missing data:\\n\", result_df.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = result_df.loc[result_df['offset'] == 0].index.tolist() # get your blank rows.\n",
    "rows_ = dict.fromkeys(result_df.columns.tolist(),'') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_new = pd.DataFrame(np.insert(result_df.values, [x for x in indices],\n",
    "                   values=list(rows_.values()), \n",
    "                   axis=0), columns=rows_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_final = result_df_new [['normalized', 'pos_tag', 'pos_tag', 'NER_tag']]\n",
    "result_df_final.columns = ['normalized', 'pos_tag', 'chunk_tag', 'NER_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df_final[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### insert -DOCSTART- row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = []\n",
    "first_row.insert(0, {'normalized': '-DOCSTART-', 'pos_tag': '-X-', 'chunk_tag': 'O', 'NER_tag' : 'O'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([pd.DataFrame(first_row), result_df_final], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_counter = [1 for i in result_df[\"NER_tag\"] if \"B-\" in i]\n",
    "print(len(ner_counter), \"named entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_final.to_csv('conll_format_file/conll.train', sep= ' ', index=False, header=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
