{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp.training import CoNLL\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(gpu=False):\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"Spark NLP - Date Extraction\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"10G\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"1000M\")\n",
    "    if gpu:\n",
    "        builder.config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.4.3\")\n",
    "    else:\n",
    "        builder.config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.3\")\n",
    "\n",
    "    return builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/home/francesco/anaconda3/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/francesco/anaconda3/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = start(gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  2.4.3\n",
      "Apache Spark version:  2.4.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_data = spark.createDataFrame([['']]).toDF('text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATE EXTRACTION PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n",
      "onto_300 download started this may take some time.\n",
      "Approximate size to download 14.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# date extraction pipeline\n",
    "document = DocumentAssembler()\\\n",
    "        .setInputCol('text')\\\n",
    "        .setOutputCol('document')\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "        .setInputCols(['document'])\\\n",
    "        .setOutputCol('sentence')\n",
    "\n",
    "token = Tokenizer()\\\n",
    "        .setInputCols(['sentence'])\\\n",
    "        .setOutputCol('token')\n",
    "\n",
    "embeddings = WordEmbeddingsModel.pretrained()\\\n",
    "        .setInputCols('sentence', 'token')\\\n",
    "        .setOutputCol('embeddings')\\\n",
    "        .setCaseSensitive(False)\n",
    "\n",
    "#bioVec_embeddings = WordEmbeddingsModel.load('./embeddings/BioVec_model')\\\n",
    "       # .setInputCols(['sentence', 'token'])\\\n",
    "       # .setOutputCol('biowordvec')\n",
    "\n",
    "nerTagger = NerDLModel.pretrained('onto_300')\\\n",
    "        .setInputCols(['sentence', 'token', 'embeddings'])\\\n",
    "        .setOutputCol('ner')\n",
    "\n",
    "nerConverter = NerConverter()\\\n",
    "        .setInputCols(['sentence', 'token', 'ner'])\\\n",
    "        .setOutputCol('ner_span')\n",
    "\n",
    "date_pipeline = Pipeline(\n",
    "        stages=[\n",
    "            document,\n",
    "            sentence,\n",
    "            token,\n",
    "            embeddings,\n",
    "            nerTagger,\n",
    "            nerConverter\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_model = date_pipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''' The patient was prescribed 1 capsule of Advil for 5 days . \n",
    "He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals ,\n",
    "and metformin 1000 mg two times a day . \n",
    "It was determined that all SGLT2 inhibitors should be discontinued indefinitely for 3 months .'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = spark.createDataFrame([[text]]).toDF('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = date_model.transform(prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = spark.createDataFrame([[\"I would like to come over and see you in 01/02/2019.\"],\n",
    "                                [\"Donald John Trump (born June 14, 1946) is the 45th and current president of the United States\"]]).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match_datetime download started this may take some time.\n",
      "Approx size to download 12.9 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "pipeline = PretrainedPipeline(\"match_datetime\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onto_recognize_entities_lg download started this may take some time.\n",
      "Approx size to download 2.3 GB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "pipeline = PretrainedPipeline(\"onto_recognize_entities_lg\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = pipeline.transform(prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|          embeddings|                 ner|            entities|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| The patient was ...|[[document, 0, 34...|[[document, 1, 58...|[[token, 1, 3, Th...|[[word_embeddings...|[[named_entity, 1...|[[chunk, 28, 28, ...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annotation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+\n",
      "|result                                                |\n",
      "+------------------------------------------------------+\n",
      "|[1, Advil, 5 days, 40, 12, 1000, two, SGLT2, 3 months]|\n",
      "+------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annotation.select(\"entities.result\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
